{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1h0NDzG2fkuS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#downsample = 차원을 줄이기위한 도구, stride가 2인 kernel의 convolutinon layer 사용\n",
        "#stride는 kernal의 도약 범위 2면 2칸씩 진행 4이면 4칸씩 뛰어넘음\n",
        "class block(nn.Module):\n",
        "  def __init__(self,in_channel,out_channel,identity_downsample=None,stride=1):\n",
        "    super(block,self).__init__()\n",
        "    self.expansion = 4\n",
        "    self.conv1 = nn.Conv2d(in_channel,out_channel,kernel_size = 1,stride=1,padding=0)\n",
        "    self.bn1 = nn.BatchNorm2d(out_channel)\n",
        "    self.conv2 = nn.Conv2d(out_channel,out_channel,kernel_size=3,stride =stride,padding=1)\n",
        "    self.bn2 = nn.BatchNorm2d(out_channel)\n",
        "    self.conv3 = nn.Conv2d(out_channel,out_channel*self.expansion,kernel_size=1,stride=1,padding=0)\n",
        "    self.bn3 = nn.BatchNorm2d(out_channel*self.expansion)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.identity_downsample =identity_downsample\n",
        "  def forward_pass(self,x):\n",
        "    identity = x\n",
        "    x = self.conv1(x)\n",
        "    x= self.bn1(x)\n",
        "    x= self.relu(x)\n",
        "    x= self.conv2(x)\n",
        "    x= self.bn2(x)      #hidden batchnorm activation 의 3겹\n",
        "    x= self.relu(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.bn3(x)\n",
        "    x= self.relu(x)\n",
        "\n",
        "    ##downsample 된 identity로 변형\n",
        "    #잔차연결의 일부 x와 downsample된 identity의 차원을 맞추는 용도\n",
        "    if self.identity_downsample is not None:\n",
        "\n",
        "      identity = self.identity_downsample(identity)\n",
        "\n",
        "    x+=identity # identity = x, 즉 conv bn relu 전의 input output을 내놓을때 input을 더 더하고 activation한뒤 return <-잔차연결\n",
        "    x=self.relu(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "class Resnet(nn.Module): #[3,4,6,3]\n",
        "  def __init__(self, block, layers,image_channel, num_classes):\n",
        "    super(Resnet,self).__init__()\n",
        "    self.in_channel = 64\n",
        "    self.conv1 = nn.Conv2d(image_channel, 64,kernel_size=7,stride = 2,padding=3)\n",
        "    self.bn1 = nn.BatchNorm2d(64)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.maxpool = nn.MaxPool2d(kernel_size = 3,stride =2,padding=1)\n",
        "\n",
        "    #ResNet layer\n",
        "    self.layer1 = self.build_layer(block,layers[0],out_channel=64,stride=1)\n",
        "    self.layer2 = self.build_layer(block,layers[1],out_channel=128,stride=2)\n",
        "    self.layer3 = self.build_layer(block,layers[2],out_channel=256,stride=2)\n",
        "    self.layer4 = self.build_layer(block,layers[3],out_channel=512,stride=2)\n",
        "\n",
        "    self.averagepool = nn.AdaptiveAvgPool2d((1,1))\n",
        "    self.fullyconnected = nn.Linear(2048,num_classes)\n",
        "\n",
        "  def forward_pass(self,x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.bn1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.layer1(x)\n",
        "    x = self.layer2(x)\n",
        "    x = self.layer3(x)\n",
        "    x = self.layer4(x)\n",
        "    x = self.averagepool(x) #(batch_size, channels, 1, 1)\n",
        "    x = x.reshape(x.shape[0],-1) #(batch_size, -1)\n",
        "    x = self.fullyconnected(x)   #(2048,num_classes)\n",
        "    return x\n",
        "\n",
        "\n",
        "  def build_layer(self,block,num_residual_blocks,out_channel,stride):\n",
        "    identity_downsample = None\n",
        "    layers = []\n",
        "\n",
        "\n",
        "\n",
        "      #nn.Sequential 은 여러 layers를 묶기 위한것\n",
        "      #걍 model =nn.Sequential()하고 안에다nn.ReLU,nn.Conv2d 이런거 다 넣으면됨\n",
        "      #어쨋든 stride가 2거나 in_channel/out_channel 4아닐때 downsample해서\n",
        "      #stride1로 만들고 in,out 비를 4로 downsample함\n",
        "\n",
        "    if stride !=1 or self.in_channel != out_channel*4:\n",
        "      identity_downsample = nn.Sequential(nn.Conv2d(self.in_channel,out_channel*4,kernel_size = 1,\n",
        "                                                    stride = stride),\n",
        "                                          nn.BatchNorm2d(out_channel*4))\n",
        "\n",
        "\n",
        "\n",
        "      #class block 사용 각각의 layer block포함 con2d,bn,relu *3\n",
        "    layers.append(block(self.in_channel,out_channel,identity_downsample,stride))\n",
        "      #하나 추가하고 update 왜냐 inchannel dim이 layer통과후 outchannel dim이 됬으므로\n",
        "      #다음 layer inchannel dim = outchannel dim 4배다\n",
        "    self.in_channel = out_channel*4\n",
        "\n",
        "    for i in range(num_residual_blocks-1):\n",
        "      layers.append(block(self.in_channel,out_channel)) #처음 256으로 증가한후 output까지 지속됨 차원\n",
        "\n",
        "    return nn.Sequential(*layers)\n",
        "    # 괄호안 * 기호 = unpacking e.g input_output =[2,3]  nn.Conv2d(*input_output) -> nn.conv2d(2,3)\n",
        "\n"
      ],
      "metadata": {
        "id": "aKYIHJazf8B8"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#residual connection\n",
        "#기본 layer , output = weight*input +bias\n",
        "#but residual conenction, output = weight*input + input + bias\n",
        "#transformer architecture에서도 사용됨\n",
        "#autoencoder의 batchnorm layer output이 decoder의 multi head attention에 input에 더해짐\n",
        "#Output=LayerNorm(Input+Sublayer(Input))\n",
        "\n",
        "\n",
        "#resnet parameter (block,layers,image_channel,num_classes)\n",
        "#layer만 조금씩 조정\n",
        "\n",
        "def Resnet50(img_channel = 3,num_classes= 1000):\n",
        "  return Resnet(block, [3,4,6,3],img_channel,num_classes)\n",
        "\n",
        "\n",
        "def Resnet50(img_channel = 3,num_classes= 1000):\n",
        "  return Resnet(block, [3,4,23,3],img_channel,num_classes)\n",
        "\n",
        "def Resnet50(img_channel = 3,num_classes= 1000):\n",
        "  return Resnet(block, [3,8,36,3],img_channel,num_classes)"
      ],
      "metadata": {
        "id": "HLhhfg_WgqdW"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}