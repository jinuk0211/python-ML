{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "v-kcnmrJPg7_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#흔히쓰는 torch함수 내부 확장판과 원리\n",
        "#model={#sequential,\n",
        "        #batchnorm1d\n",
        "        #tanh}"
      ],
      "metadata": {
        "id": "0LGctwCBg0s0"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N36FGv9y6NEr"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = open('names.txt','r').read().splitlines()\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)} #i+1 인이유 enumerate의 첫 index - 0이고 이를 시작토큰 .으로 설정\n",
        "stoi['.'] = 0\n",
        "itos = {i+1:s for i,s in enumerate(chars)} # {i:s for i,s in stoi.items()} 도 가능\n",
        "itos[0] = '.'\n",
        "itos\n",
        "#데이터셋 만들기\n",
        "block_size = 3 #context length 몇개의 알파벳을 다음문자를 예측하기 위해 쓸거냐\n",
        "def build_dataset(words):\n",
        "  x,y=[],[]\n",
        "  for w in words:\n",
        "    context = [0] * block_size\n",
        "    for ch in w + '.':                #olivia [0,0,0]\n",
        "      ix =stoi[ch]                    #           o,  l,  i,  v,  i,  a\n",
        "      x.append(context)               #[[0,0,0]] ...,..o,.ol,oli,liv,iva,...\n",
        "      y.append(ix)                    #[15] <= olivia 의 stoi가 들어감\n",
        "      context = context[1:] +[ix]     # 값전달 list\n",
        "  #build_dataset(words[:3])\n",
        "  #\n",
        "  x=torch.tensor(x)\n",
        "  y=torch.tensor(y)\n",
        "  print(x.shape,y.shape)\n",
        "\n",
        "  return x,y\n",
        "\n",
        "build_dataset(words[:3])\n",
        "\n",
        "import random\n",
        "random.seed(42)\n",
        "random.shuffle(words)\n",
        "\n",
        "n1 = int(0.8*len(words))\n",
        "n2 = int(0.9*len(words))\n",
        "\n",
        "xtr,ytr = build_dataset(words[:n1])\n",
        "xval, yval = build_dataset(words[n1:n2])\n",
        "xtest, ytest = build_dataset(words[n2:])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeTDVQXWuHUi",
        "outputId": "2e0359a7-0611-48a4-ceeb-5b7893cd4ac0"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 3]) torch.Size([16])\n",
            "torch.Size([182625, 3]) torch.Size([182625])\n",
            "torch.Size([22655, 3]) torch.Size([22655])\n",
            "torch.Size([22866, 3]) torch.Size([22866])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear:\n",
        "\n",
        "  #parameter\n",
        "  def __init__(self,fanin,fanout,bias =True):\n",
        "    self.weight = torch.randn((fanin,fanout),generator = g)/ fanin**0.5\n",
        "    #fanout 은 hidden layer뉴런개수\n",
        "    #fanin 은 input의 가공된 tensor\n",
        "    self.bias = torch.randn((fanout)) if bias else None\n",
        "\n",
        "  #linear 안에서 벌어지는 일\n",
        "  def __call__(self,x):\n",
        "    self.out = x @ self.weight\n",
        "    if self.bias is not None:\n",
        "      self.out += self.bias\n",
        "    return self.out\n",
        "\n",
        "  #backprop을 위한 parameter 등록\n",
        "  def parameter(self):\n",
        "    return [self.weight] + ([] if self.bias is None else [self.bias])\n",
        "\n",
        "#----------------------------------------------------------------------\n",
        "\n",
        "class batchnorm1d:\n",
        "\n",
        "  def __init__(self,dim,eps=1e-5,momentum=0.1):\n",
        "    # eps 는 batchnorm 과정중 분모가 0이 되는 걸 막는 작은값\n",
        "\n",
        "    self.eps =  eps\n",
        "    self.momentum = momentum\n",
        "    self.training = True\n",
        "    #parameter\n",
        "\n",
        "    #gamma 값이 1인 이유= 직선식에서 ax+b 부분의 a부분이기 때문 초기화 값 당연히 1\n",
        "    #beta 값이 1인이유 = 직선식에서 ax+b 부분의 b부분의 초기화 값이기 때문\n",
        "    #gamma beta는 scale shift를 위한것 다양한 값을 선택하기 위함\n",
        "    #gaussian 분포의 x,y축으로의 움직임\n",
        "\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "\n",
        "    #buffer\n",
        "    #위와비슷 초기 mean이 0으로 선택되면 고려x\n",
        "    #variance또한 마찬가지 1로 설정되면 고려x\n",
        "    self.runningmean = torch.zeros(dim)\n",
        "    self.runningvar= torch.ones(dim)\n",
        "\n",
        "  def __call__(self,x):\n",
        "\n",
        "    if self.training:\n",
        "      xmean = x.mean(0,keepdim=True)  #batchmean\n",
        "      xvar =x.var(0,keepdim=True,unbiased=True)   #batchvariance\n",
        "    else:\n",
        "      xmean = self.runningmean\n",
        "      xvar= self.runningvar\n",
        "\n",
        "    xhat=(x-xmean)/torch.sqrt(self.eps+xvar) #actiavationm,batchnorm.ipynb\n",
        "    self.out = self.gamma*xhat + self.beta\n",
        "\n",
        "    #buffer 업데이트  momentum comes into play & gradient descent 영향을 받지않음\n",
        "    #부드럽게 업데이트할 수 있으며, 훈련 초기에는 높은 학습률을 유지함\n",
        "    #훈련이 진행될수록 이동 평균이 더 중요해지게 됨\n",
        "\n",
        "    #전체 데이터셋 평균,분산만 사용하는게 아니라 학습하며 얻은정보도 활용\n",
        "    if self.training:\n",
        "      with torch.no_grad():\n",
        "        self.runningmean =  (1-self.momentum)*self.runningmean + self.mementum*xmean\n",
        "        self.runningvar = (1-self.momentum)*self.runningvar + self.momentum*xvar\n",
        "    return self.out\n",
        "\n",
        "    def parameter(self):\n",
        "      return [self.gamma,self.beta ]\n",
        "\n",
        "#----------------------------------------------------------------------------------\n",
        "\n",
        "class tanh:\n",
        "  def __call__(self,x):\n",
        "    self.out = torch.tanh(x)\n",
        "    return self.out\n",
        "\n",
        "  def parameter(self):\n",
        "    return []\n",
        "\n",
        "n_embedding = 10\n",
        "block_size = 3\n",
        "vocab_size = 27\n",
        "n_hidden = 200\n",
        "g= torch.Generator().manual_seed(2147483647)\n",
        "\n",
        "C= torch.randn((vocab_size,n_embedding),generator=g)\n",
        "\n",
        "layers = [\n",
        "  Linear(n_embedding*block_size,n_hidden),tanh(),\n",
        "  Linear(              n_hidden,n_hidden),tanh(),\n",
        "  Linear(              n_hidden,n_hidden),tanh(),\n",
        "  Linear(              n_hidden,n_hidden),tanh(),\n",
        "  Linear(              n_hidden,n_hidden),tanh(),\n",
        "  Linear(              n_hidden,vocab_size),\n",
        "]\n",
        "\n",
        "with torch.no_grad():\n",
        "  #마지막 layer less confident\n",
        "  layers[-1].weight *= 0.1\n",
        "  #모든 layer에 gain 적용 -> linear layer값의 std조정에 영향\n",
        "  for layer in layers:\n",
        "    if isinstance(layer,Linear):\n",
        "      layer.weight *= 5/3 ################## 밑의 plt 그래프와 연관 # *= 5/3으로 바꿔보자\n",
        "\n",
        "parameters =  [C] + [p for layer in layers for p in layer.parameter()]\n",
        "#tanh, linear의 모든 parameter 개수\n",
        "print(sum(p.nelement() for p in parameters))\n",
        "for p in parameters:\n",
        "  p.requires_grad= True\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86dFMfsZPsDS",
        "outputId": "312e50c0-8163-4221-c158-3ab0eb3cad61"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "172697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_step = 100000\n",
        "batch_size = 32\n",
        "lossi = []\n",
        "ud= [] # update to data ratio gradient descent 가 data 에 얼마나 영향을 주었는가\n",
        "\n",
        "#\n",
        "#-----------------------------------------------------------\n",
        "for i in range(max_step):\n",
        "\n",
        "  idx = torch.randint(0,xtr.shape[0],(batch_size,),generator = g)\n",
        "  xb,yb = xtr[idx],ytr[idx]\n",
        "\n",
        "  #forward pass\n",
        "#--------------------------------------------------------------------------\n",
        "\n",
        "  emb = C[xb]\n",
        "  # xb shape = 32,3 C shape = 27,10 C가 뭐였냐 -> embedding matrix\n",
        "  #emb shape = 32,3,10 pytorch의 tensor 에 tensor index 원리? 몰루\n",
        "  x = emb.view(emb.shape[0],-1)\n",
        "  # emb.shape[0] = 32 즉 x shape = 32,30\n",
        "  #x (batch_size, block_size*n_embedding)\n",
        "  for layer in layers:\n",
        "    x = layer(x)\n",
        "  loss = F.cross_entropy(x,yb)\n",
        "\n",
        "  #backward pass\n",
        "#--------------------------------------------------------------------------\n",
        "\n",
        "  #gradient 가 순전파 forward passm,역전파 backward pass 계속 쌓여서\n",
        "  # tensor 의 gradient과정을 역추적 과정 디버깅에 사용\n",
        "  #메모리 사용량이 매우 증가할수 있음!\n",
        "\n",
        "  #for layer in layers:\n",
        "    #layer.out.retain_grad()\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "  loss.backward()\n",
        "\n",
        "\n",
        "\n",
        "  #update\n",
        "#---------------------------------------------------------------------------\n",
        "\n",
        "    #update learnining rate\n",
        "  lr = 0.1 if i < 100000 else 0.01\n",
        "  #학습률 조절 초기엔 빠르게 갈수록 안정적으로\n",
        "  for p in parameters:\n",
        "    p.data += -lr * p.grad\n",
        "\n",
        "\n",
        "\n",
        "  if i % 10000 == 0:\n",
        "    print(f'{i:7d}/{max_step:7d}: {loss.item():.4f}')\n",
        "  lossi.append(loss.log10().item())\n",
        "  with torch.no_grad():\n",
        "    ud.append([(lr*p.grad.std() / p.data.std()).log10().item() for p in parameters])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeWEtDv9r3I6",
        "outputId": "58655e2c-84d4-4824-eeac-01b5eea4d6f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      0/ 100000: 3.5824\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(20,4))\n",
        "legends=[]\n",
        "for i,layer in enumerate(layers[:-1]): #마지막 output tanh만 뺴고\n",
        "  if isinstance(layer,tanh):\n",
        "    t= layer.out\n",
        "    print('layer %d (%10s): mean %+.2f,std %.2f, saturated: %2.f%%'%(i,layer.__class__.__name__,t.mean(),t.std(),(t.abs()>0.97).float().mean()*100))\n",
        "    hy,hx = torch.histogram(t,density=True)\n",
        "    plt.plot(hx[:-1].detach(),hy.detach())\n",
        "    legends.append(f'layer {i}({layer.__class__.__name__}')\n",
        "plt.legend(legends);\n",
        "plt.title('activation distribution')\n",
        "\n",
        "#activation distribution이 일정한이유 5/3 / sqrt(fan_in)을 곱해줬기 떄문"
      ],
      "metadata": {
        "id": "Cd6oevXE5jgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.figure(figsize=(20,4))\n",
        "legends=[]\n",
        "for i,layer in enumerate(layers[:-1]): #마지막 output tanh만 뺴고\n",
        "  if isinstance(layer,Linear):\n",
        "    t= layer.out\n",
        "    print('layer %d (%10s): mean %+.2f,std %.2f, saturated: %2.f%%'%(i,layer.__class__.__name__,t.mean(),t.std(),(t.abs()>0.97).float().mean()*100))\n",
        "    hy,hx = torch.histogram(t,density=True)\n",
        "    plt.plot(hx[:-1].detach(),hy.detach())\n",
        "    legends.append(f'layer {i}({layer.__class__.__name__}')\n",
        "plt.legend(legends);\n",
        "plt.title('gradient distribution')\n",
        "\n",
        "#activation distribution이 일정한이유 5/3 / sqrt(fan_in)을 곱해줬기 떄문"
      ],
      "metadata": {
        "id": "OfiKniTNDZi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%Ismagic\n"
      ],
      "metadata": {
        "id": "EaNw0xhMdy-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "plt.figure(figsize=(20,4))\n",
        "legends= []\n",
        "for i, p in enumerate(parameters):\n",
        "  if p.ndim ==2: # weight를 의미\n",
        "  # ud.append([(lr*p.grad.std() / p.data.std()).log10().item() for p in parameters])\n",
        "    plt.plot([ud[j][i] for j in range(len(ud))])\n",
        "  # weight의 update data 비율만을 출력한다는 뜻\n",
        "    legends.append('parameter %d' % i)\n",
        "plt.plot([0,len(ud)],[-3,-3],'k')\n",
        "plt.legend(legends);"
      ],
      "metadata": {
        "id": "WL1E6KQqQv8-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}