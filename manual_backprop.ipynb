{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "LmJebJQ5fiVw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "#file_path = '/content/drive/MyDrive/names.txt'\n"
      ],
      "metadata": {
        "id": "ue26xf01WWua"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = open('names.txt','r').read().splitlines()\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)} #i+1 인이유 enumerate의 첫 index - 0이고 이를 시작토큰 .으로 설정\n",
        "stoi['.'] = 0\n",
        "itos = {i+1:s for i,s in enumerate(chars)} # {i:s for i,s in stoi.items()} 도 가능\n",
        "itos[0] = '.'\n",
        "\n",
        "vocab_size = len(itos)\n",
        "\n",
        "#데이터셋 만들기\n",
        "block_size = 3 #context length 몇개의 알파벳을 다음문자를 예측하기 위해 쓸거냐\n",
        "def build_dataset(words):\n",
        "  x,y=[],[]\n",
        "  for w in words:\n",
        "    context = [0] * block_size\n",
        "    for ch in w + '.':                #olivia [0,0,0]\n",
        "      ix =stoi[ch]                    #           o,  l,  i,  v,  i,  a\n",
        "      x.append(context)               #[[0,0,0]] ...,..o,.ol,oli,liv,iva,...\n",
        "      y.append(ix)                    #[15] <= olivia 의 stoi가 들어감\n",
        "      context = context[1:] +[ix]     # 값전달 list\n",
        "  #build_dataset(words[:3])\n",
        "  #\n",
        "  x=torch.tensor(x)\n",
        "  y=torch.tensor(y)\n",
        "  print(x.shape,y.shape)\n",
        "\n",
        "  return x,y\n",
        "\n",
        "build_dataset(words[:3])\n",
        "\n",
        "import random\n",
        "random.seed(42)\n",
        "random.shuffle(words)\n",
        "\n",
        "n1 = int(0.8*len(words))\n",
        "n2 = int(0.9*len(words))\n",
        "\n",
        "xtr,ytr = build_dataset(words[:n1])\n",
        "xval, yval = build_dataset(words[n1:n2])\n",
        "xtest, ytest = build_dataset(words[n2:])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZmioHDYhL8x",
        "outputId": "ac9eeb5a-cb08-4ab6-96a2-8a8972067bc4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 3]) torch.Size([16])\n",
            "torch.Size([182625, 3]) torch.Size([182625])\n",
            "torch.Size([22655, 3]) torch.Size([22655])\n",
            "torch.Size([22866, 3]) torch.Size([22866])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yj04d9gWmVn1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#utility function\n",
        "# compare manual gradients to pytorch gradient의 줄임말\n",
        "# 직접만든 backward와 pytorch backword를 비교해보자\n",
        "\n",
        "def cmp(s, dt, t):\n",
        "  ex = torch.all(dt == t.grad).item()\n",
        "  app = torch.allclose(dt, t.grad)\n",
        "  maxdiff = (dt - t.grad).abs().max().item()\n",
        "  print(f'{s:15s}|실제값: {str(ex):5s}| 추정값: {str(app):5s}| 최대차이값:{maxdiff}')\n"
      ],
      "metadata": {
        "id": "V2z4Y3acPcq2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#activation, batchnorm ipynb\n",
        "n_embedding = 10\n",
        "n_hidden = 64\n",
        "\n",
        "g= torch.Generator().manual_seed(2147483647)\n",
        "\n",
        "C= torch.randn((vocab_size,n_embedding),generator=g)\n",
        "# *gain/sqrt(fan_in) std 조정\n",
        "w1 = torch.randn((n_embedding*block_size,n_hidden), generator= g)*5/3/(n_embedding*block_size)**0.5\n",
        "#batchnorm 때문에 써도 되고 안써도됨\n",
        "#b1 = torch.randn(n_hidden,generator =g) * 0.1\n",
        "\n",
        "w2= torch.randn((n_hidden,vocab_size),generator=g) * 0.1   #hpreact.abs()줄이기\n",
        "b2 = torch.randn(vocab_size, generator=g)*0.1\n",
        "\n",
        "bngain = torch.randn((1,n_hidden))*0.1+ 1.0\n",
        "bnbias = torch.randn((1,n_hidden))*0.1\n",
        "\n",
        "parameters = [C,w1,w2,b2,bngain,bnbias]\n",
        "print(sum(p.nelement() for p in parameters)) #param 총개수\n",
        "for p in parameters:\n",
        "  p.requires_grad = True\n",
        "  #같은 snippet as always"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MsE7cI9ypH50",
        "outputId": "70aa16ab-06ae-4d19-fcdf-220599e552b6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4073\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "n= batch_size # 편의를 위해서\n",
        "\n",
        "## mini batch 만들기\n",
        "#batch size, 하면 size[32]가되고 , 없으면 size[1]이 됨\n",
        "idx = torch.randint(0,xtr.shape[0],(batch_size,),generator = g)\n",
        "xb,yb = xtr[idx],ytr[idx]\n",
        "#xb shape 32,3으로 xtr의 182625,3 tensor를 batch size 32씩 나눈것\n",
        "\n",
        "# forward pass\n",
        "#---------------------------------------------------------------\n",
        "emb = C[xb] #embed the characters into vector\n",
        "# xb shape = 32,3 C shape = 27,10 C가 뭐였냐 -> embedding matrix\n",
        "#emb shape = 32,3,10 pytorch의 tensor 에 tensor index 원리? 생략\n",
        "embconcat = emb.view(emb.shape[0],-1) #concatanate vectors\n",
        "# emb.shape[0] = 32 즉 embconcat shape = 32,30\n",
        "\n",
        "#linear layer | first hidden layer\n",
        "#--------------------------------------------------\n",
        "hprebn = embconcat @ w1\n",
        "# hidden pre activation batch normalization 간단히 batchnorm 전 input\n",
        "\n",
        "#batchnorm | batch norm layer\n",
        "#---------------------------------------------------------\n",
        "bnmean = hprebn.sum(0,keepdim=True)/n #평균\n",
        "bndiff = hprebn - bnmean #편차\n",
        "bndiff2 = bndiff**2 #편차제곱\n",
        "bnvar = bndiff2.sum(0,keepdim=True)/(n-1) #bossel's correlation 샘플 표준편차,분산은 n-1로 나눔\n",
        "bnvar_inv = (bnvar + 1e-5)**-0.5 #분모부분 inv = inverted\n",
        "\n",
        "\n",
        "bnraw = bndiff * bnvar_inv # gamma beta 로 scale shift 하기전의 값\n",
        "hpreact = bngain * bnraw + bnbias\n",
        "\n",
        "# BATCH NORMALIZATION 식 1e-5 는 엡실론으로 분모 0 방지\n",
        "# 빠른 방법 -> hpreact = bngain *(bndiff/(torch.sqrt(bnvar + 1e-55))  ) + bnbias\n",
        "# 하지만 모든 parameter grad의 역전파과정 살펴보는게 목적\n",
        "#activation | tanh layer\n",
        "#-----------------------------------------------------\n",
        "h = torch.tanh(hpreact)\n",
        "\n",
        "#linear | second hidden layer\n",
        "#---------------------------------------------------\n",
        "logits = h @ w2 + b2\n",
        "\n",
        "#loss function | cross entropy  손으로\n",
        "#--------------------------------------------------------\n",
        "\n",
        "# F.cross_entropy(logits,yb) 와 100% 똑같음\n",
        "# pytorch cross entropy 모듈 내부\n",
        "logit_maxes = logits.max(1, keepdim=True).values\n",
        "norm_logits = logits - logit_maxes #batch norm한 것과 유사 stabilizing\n",
        "counts = norm_logits.exp()\n",
        "counts_sum = counts.sum(1, keepdims=True)\n",
        "counts_sum_inv = counts_sum**-1\n",
        "\n",
        "#(1.0 / counts_sum)을 사용한다면 backprop을 정확하게 할수없음\n",
        "\n",
        "probs = counts * counts_sum_inv #결국 확률값\n",
        "logprobs = probs.log()  #\n",
        "loss = -logprobs[range(n), yb].mean()\n",
        "#xb 매우많은 input 을 32개씩 쪼갠 32,3shape [[o,l,i][l,i,v],[i,v,a],..] 의 stoi index\n",
        "#yb 매우많은 output을 32개씩 쪼갠 32 shape [o,l,i,v,i,a,...] 의 stoi index\n",
        "\n",
        "#pytorch의 backward()사용한 backward pass not manually\n",
        "for p in parameters:\n",
        "  p.grad = None\n",
        "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
        "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
        "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmean,\n",
        "         embconcat, emb]:\n",
        "\n",
        "  t.retain_grad()\n",
        "loss.backward()\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sam5tc3gt7zh",
        "outputId": "a2ce69d6-9ec3-4b7b-81da-9dde47fb71e5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3.4670, grad_fn=<NegBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#------------------------------\n",
        "# yb shape 32\n",
        "#dloss/da = - 1/n\n",
        "#loss = -1/3a -1/3b -1/3c 이유 -(a+b+c)/n -logprobs[range(n), yb].mean()\n",
        "#loss는 평균을 구한 값이기에 gradient도 덧셈에서의 상수값\n",
        "#logprob의 값변화자체는 gradient에 영향을 주지않는다\n",
        "dlogprobs =  torch.zeros_like(logprobs)\n",
        "dlogprobs[range(n),yb] = -1.0/n\n",
        "cmp('logprobs',dlogprobs,logprobs) # pytorch의 logprobs.grad와 직접구한 gradient 차이\n",
        "#--------------------------------------------------------\n",
        "dprobs = (1.0/probs) * dlogprobs\n",
        "# chain rule prob 미분식 dloss/dlogprob * dlogprob/dprob , logprob = dprob.log(), dlogprob/dprob = 1/prob\n",
        "cmp('probs',dprobs,probs)\n",
        "#------------------------------------------------\n",
        "#counts shape = [32,27]\n",
        "#counts_inv shape = [32,1]\n",
        "#32,27 matrix에 가로줄에 counts inv 의 32 row 값이 모두 곱해짐\n",
        "#probs = counts * counts_sum_inv  #dprobs/dcounts = counts_sum_inv\n",
        "# 다른 shape의 multiplication의 derivitve\n",
        "dcounts = counts_sum_inv * dprobs\n",
        "dcounts_sum_inv =  (dprobs * counts).sum(1,keepdim=True)\n",
        "cmp('dcounts_sum_inv',dcounts_sum_inv,counts_sum_inv)\n",
        "#이하 머리아파서 추후에"
      ],
      "metadata": {
        "id": "zxoSXmMfBbfq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc8dfeaf-68df-42b8-f61b-bd89e0558b2d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logprobs       |실제값: True | 추정값: True | 최대차이값:0.0\n",
            "probs          |실제값: True | 추정값: True | 최대차이값:0.0\n",
            "dcounts_sum_inv|실제값: True | 추정값: True | 최대차이값:0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#bessel's correlation은 minibatch와 같은 적은량의 dataset의 분산을 구할때 효과적\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqNKBnpMePGR",
        "outputId": "4e9a7664-ede1-433c-809e-a83c06ebb05e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3.4670, grad_fn=<NegBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#cross entropy 역전파 직접작성\n",
        "fast_loss = F.cross_entropy(logits,yb)\n",
        "print(fast_loss.item(),'diff:',(fast_loss - loss).item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rM7PS1UsV3_x",
        "outputId": "618c29c9-c21a-4ea8-d20f-d781234cd1cd"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.4669768810272217 diff: 7.152557373046875e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#batchnorm backward manually"
      ],
      "metadata": {
        "id": "D8OCsB4MaFlk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}