{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "LmJebJQ5fiVw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = open('names.txt','r').read().splitlines()\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)} #i+1 인이유 enumerate의 첫 index - 0이고 이를 시작토큰 .으로 설정\n",
        "stoi['.'] = 0\n",
        "itos = {i+1:s for i,s in enumerate(chars)} # {i:s for i,s in stoi.items()} 도 가능\n",
        "itos[0] = '.'\n",
        "\n",
        "vocab_size = len(itos)\n",
        "\n",
        "#데이터셋 만들기\n",
        "block_size = 3 #context length 몇개의 알파벳을 다음문자를 예측하기 위해 쓸거냐\n",
        "def build_dataset(words):\n",
        "  x,y=[],[]\n",
        "  for w in words:\n",
        "    context = [0] * block_size\n",
        "    for ch in w + '.':                #olivia [0,0,0]\n",
        "      ix =stoi[ch]                    #           o,  l,  i,  v,  i,  a\n",
        "      x.append(context)               #[[0,0,0]] ...,..o,.ol,oli,liv,iva,...\n",
        "      y.append(ix)                    #[15] <= olivia 의 stoi가 들어감\n",
        "      context = context[1:] +[ix]     # 값전달 list\n",
        "  #build_dataset(words[:3])\n",
        "  #\n",
        "  x=torch.tensor(x)\n",
        "  y=torch.tensor(y)\n",
        "  print(x.shape,y.shape)\n",
        "\n",
        "  return x,y\n",
        "\n",
        "build_dataset(words[:3])\n",
        "\n",
        "import random\n",
        "random.seed(42)\n",
        "random.shuffle(words)\n",
        "\n",
        "n1 = int(0.8*len(words))\n",
        "n2 = int(0.9*len(words))\n",
        "\n",
        "xtr,ytr = build_dataset(words[:n1])\n",
        "xval, yval = build_dataset(words[n1:n2])\n",
        "xtest, ytest = build_dataset(words[n2:])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "bZmioHDYhL8x",
        "outputId": "685fcbb9-ed10-46d0-9883-932ca9a018ba"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 3]) torch.Size([16])\n",
            "torch.Size([182625, 3]) torch.Size([182625])\n",
            "torch.Size([22655, 3]) torch.Size([22655])\n",
            "torch.Size([22866, 3]) torch.Size([22866])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#utility function\n",
        "# compare manual gradients to pytorch gradient의 줄임말\n",
        "# 직접만든 backward와 pytorch backword를 비교해보자\n",
        "\n",
        "def cmp(s,dt,t):\n",
        "  ex = torch.all(dt==t.grad).item()\n",
        "  app = torch.allclose(dt,t.grad)\n",
        "  maxdiff = (dt-t.grad).abs().max().item()\n",
        "  print(f'{s:15s}|실제값: {str(ex):5s}| 추정값: {str(app):5s}| 최대차이값:{maxdiff}')"
      ],
      "metadata": {
        "id": "yj04d9gWmVn1"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#activation, batchnorm ipynb\n",
        "n_embedding = 10\n",
        "n_hidden = 64\n",
        "\n",
        "g= torch.Generator().manual_seed(2147483647)\n",
        "\n",
        "C= torch.randn((vocab_size,n_embedding),generator=g)\n",
        "# *gain/sqrt(fan_in) std 조정\n",
        "w1 = torch.randn((n_embedding*block_size,n_hidden), generator= g)*5/3/(n_embedding*block_size)**0.5\n",
        "#batchnorm 때문에 써도 되고 안써도됨\n",
        "#b1 = torch.randn(n_hidden,generator =g) * 0.1\n",
        "\n",
        "w2= torch.randn((n_hidden,vocab_size),generator=g) * 0.1   #hpreact.abs()줄이기\n",
        "b2 = torch.randn(vocab_size, generator=g)*0.1\n",
        "\n",
        "bngain = torch.randn((1,n_hidden))*0.1+ 1.0\n",
        "bnbias = torch.randn((1,n_hidden))*0.1\n",
        "\n",
        "parameters = [C,w1,w2,b2,bngain,bnbias]\n",
        "print(sum(p.nelement() for p in parameters)) #param 총개수\n",
        "for p in parameters:\n",
        "  p.requires_grad = True\n",
        "  #같은 snippet as always"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "MsE7cI9ypH50",
        "outputId": "142a4360-c881-4318-d54e-c89122f01144"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4073\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "n= batch_size # 편의를 위해서\n",
        "\n",
        "## mini batch 만들기\n",
        "#batch size, 하면 size[32]가되고 , 없으면 size[1]이 됨\n",
        "idx = torch.randint(0,xtr.shape[0],(batch_size,),generator = g)\n",
        "xb,yb = xtr[idx],ytr[idx]\n",
        "#xb shape 32,3으로 xtr의 182625,3 tensor를 batch size 32씩 나눈것\n",
        "\n",
        "# forward pass\n",
        "#---------------------------------------------------------------\n",
        "emb = C[xb] #embed the characters into vector\n",
        "# xb shape = 32,3 C shape = 27,10 C가 뭐였냐 -> embedding matrix\n",
        "#emb shape = 32,3,10 pytorch의 tensor 에 tensor index 원리? 생략\n",
        "embconcat = emb.view(emb.shape[0],-1) #concatanate vectors\n",
        "# emb.shape[0] = 32 즉 embconcat shape = 32,30\n",
        "\n",
        "#linear layer | first hidden layer\n",
        "#--------------------------------------------------\n",
        "hprebn = embconcat @ w1\n",
        "# hidden pre activation batch normalization 간단히 batchnorm 전 input\n",
        "\n",
        "#batchnorm | batch norm layer\n",
        "#---------------------------------------------------------\n",
        "bnmean = hprebn.sum(0,keepdim=True)/n #평균\n",
        "bndiff = hprebn - bnmean #편차\n",
        "bndiff2 = bndiff**2 #편차제곱\n",
        "bnvar = bndiff2.sum(0,keepdim=True)/(n-1) #bossel's correlation 샘플 표준편차,분산은 n-1로 나눔\n",
        "bnvar_inv = (bnvar + 1e-5)**-0.5 #분모부분 inv = inverted\n",
        "\n",
        "\n",
        "bnraw = bndiff * bnvar_inv # gamma beta 로 scale shift 하기전의 값\n",
        "hpreact = bngain * bnraw + bnbias\n",
        "\n",
        "# BATCH NORMALIZATION 식 1e-5 는 엡실론으로 분모 0 방지\n",
        "# 빠른 방법 -> hpreact = bngain *(bndiff/(torch.sqrt(bnvar + 1e-55))  ) + bnbias\n",
        "# 하지만 모든 parameter grad의 역전파과정 살펴보는게 목적\n",
        "#activation | tanh layer\n",
        "#-----------------------------------------------------\n",
        "h = torch.tanh(hpreact)\n",
        "\n",
        "#linear | second hidden layer\n",
        "#---------------------------------------------------\n",
        "logits = h @ w2 + b2\n",
        "\n",
        "#loss function | cross entropy  손으로\n",
        "#--------------------------------------------------------\n",
        "\n",
        "# F.cross_entropy(logits,yb) 와 100% 똑같음\n",
        "# pytorch cross entropy 모듈 내부\n",
        "logit_maxes = logits.max(1, keepdim=True).values\n",
        "norm_logits = logits - logit_maxes #batch norm한 것과 유사 stabilizing\n",
        "counts = norm_logits.exp()\n",
        "counts_sum = counts.sum(1, keepdims=True)\n",
        "counts_sum_inv = counts_sum**-1\n",
        "\n",
        "#(1.0 / counts_sum)을 사용한다면 backprop을 정확하게 할수없음\n",
        "\n",
        "probs = counts * counts_sum_inv #결국 확률값\n",
        "logprobs = probs.log()  #\n",
        "loss = -logprobs[range(n), yb].mean()\n",
        "#xb 매우많은 input 을 32개씩 쪼갠 32,3shape [[o,l,i][l,i,v],[i,v,a],..] 의 stoi index\n",
        "#yb 매우많은 output을 32개씩 쪼갠 32 shape [o,l,i,v,i,a,...] 의 stoi index\n",
        "\n",
        "#pytorch의 backward()사용한 backward pass not manually\n",
        "for p in parameters:\n",
        "  p.grad = None\n",
        "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
        "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
        "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmean,\n",
        "         embconcat, emb]:\n",
        "\n",
        "  t.retain_grad()\n",
        "loss.backward()\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Sam5tc3gt7zh",
        "outputId": "b8385358-ccdb-481a-ce46-321400b23527"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3.4987, grad_fn=<NegBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "n= batch_size # 편의를 위해서\n",
        "\n",
        "## mini batch 만들기\n",
        "#batch size, 하면 size[32]가되고 , 없으면 size[1]이 됨\n",
        "idx = torch.randint(0,xtr.shape[0],(batch_size,),generator = g)\n",
        "xb,yb = xtr[idx],ytr[idx]\n",
        "#xb shape 32,3으로 xtr의 182625,3 tensor를 batch size 32씩 나눈것\n",
        "\n",
        "# forward pass\n",
        "#---------------------------------------------------------------\n",
        "emb = C[xb] #embed the characters into vector\n",
        "# xb shape = 32,3 C shape = 27,10 C가 뭐였냐 -> embedding matrix\n",
        "#emb shape = 32,3,10 pytorch의 tensor 에 tensor index 원리? 생략\n",
        "embconcat = emb.view(emb.shape[0],-1) #concatanate vectors\n",
        "# emb.shape[0] = 32 즉 embconcat shape = 32,30\n",
        "\n",
        "#linear layer | first hidden layer\n",
        "#--------------------------------------------------\n",
        "hprebn = embconcat @ w1\n",
        "# hidden pre activation batch normalization 간단히 batchnorm 전 input\n",
        "\n",
        "#batchnorm | batch norm layer\n",
        "#---------------------------------------------------------\n",
        "bnmean = hprebn.sum(0,keepdim=True)/n #평균\n",
        "bndiff = hprebn - bnmean #편차\n",
        "bndiff2 = bndiff**2 #편차제곱\n",
        "bnvar = bndiff2.sum(0,keepdim=True)/n-1 #bossel's correlation 샘플 표준편차,분산은 n-1로 나눔\n",
        "\n",
        "# BATCH NORMALIZATION 식 1e-5 는 엡실론으로 분모 0 방지\n",
        "hpreact = bngain *(bndiff/(torch.sqrt(bnvar + 1e-55))  ) + bnbias\n",
        "#hidden pre activation -> non linearty tanh\n",
        "#activation | tanh layer\n",
        "#-----------------------------------------------------\n",
        "x = torch.tanh(hpreact)\n",
        "b2.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "iHwOgJNs_Tpr",
        "outputId": "a467b0c6-3a75-47e2-a662-35c8a50a440c"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([27])"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zxoSXmMfBbfq"
      },
      "execution_count": 82,
      "outputs": []
    }
  ]
}