{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "YNAxQNtoY8Jw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = open('names.txt','r').read().splitlines()\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)} #i+1 인이유 enumerate의 첫 index - 0이고 이를 시작토큰 .으로 설정\n",
        "stoi['.'] = 0\n",
        "itos = {i+1:s for i,s in enumerate(chars)} # {i:s for i,s in stoi.items()} 도 가능\n",
        "itos[0] = '.'\n",
        "itos\n",
        "#데이터셋 만들기\n",
        "vocab_size = 27\n",
        "block_size = 8 #context length 몇개의 알파벳을 다음문자를 예측하기 위해 쓸거냐\n",
        "def build_dataset(words):\n",
        "  x,y=[],[]\n",
        "  for w in words:\n",
        "    context = [0] * block_size\n",
        "    for ch in w + '.':                #olivia [0,0,0]\n",
        "      ix =stoi[ch]                    #           o,  l,  i,  v,  i,  a\n",
        "      x.append(context)               #[[0,0,0]] ...,..o,.ol,oli,liv,iva,...\n",
        "      y.append(ix)                    #[15] <= olivia 의 stoi가 들어감\n",
        "      context = context[1:] +[ix]     # 값전달 list\n",
        "  #build_dataset(words[:3])\n",
        "  #\n",
        "  x=torch.tensor(x)\n",
        "  y=torch.tensor(y)\n",
        "  print(x.shape,y.shape)\n",
        "\n",
        "  return x,y\n",
        "\n",
        "build_dataset(words[:3])\n",
        "\n",
        "import random\n",
        "random.seed(42)\n",
        "random.shuffle(words)\n",
        "\n",
        "n1 = int(0.8*len(words))\n",
        "n2 = int(0.9*len(words))\n",
        "\n",
        "xtr,ytr = build_dataset(words[:n1])\n",
        "xval, yval = build_dataset(words[n1:n2])\n",
        "xtest, ytest = build_dataset(words[n2:])\n"
      ],
      "metadata": {
        "id": "VnqFX3BgZDQf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47ac83c4-4a63-4404-a002-d97d5c0b60a2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 8]) torch.Size([16])\n",
            "torch.Size([182625, 8]) torch.Size([182625])\n",
            "torch.Size([22655, 8]) torch.Size([22655])\n",
            "torch.Size([22866, 8]) torch.Size([22866])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear:\n",
        "    def __init__(self, fan_in, fan_out, bias = True):\n",
        "        self.weight = torch.randn((fan_in, fan_out))\n",
        "        self.weight /= fan_in ** 0.5\n",
        "        self.bias = torch.zeros((fan_out)) if bias else None\n",
        "\n",
        "    def __call__(self, x):\n",
        "        self.out = x @ self.weight\n",
        "        if self.bias is not None:\n",
        "            self.out += self.bias\n",
        "        return self.out\n",
        "\n",
        "    def parameter(self):\n",
        "        return [self.weight] + ([] if self.bias is None else [self.bias])\n",
        "\n",
        "\n",
        "\n",
        "#----------------------------------------------------------------------\n",
        "\n",
        "class batchnorm1d:\n",
        "\n",
        "  def __init__(self,dim,eps=1e-5,momentum=0.1):\n",
        "    # eps 는 batchnorm 과정중 분모가 0이 되는 걸 막는 작은값\n",
        "\n",
        "    self.eps =  eps\n",
        "    self.momentum = momentum\n",
        "    self.training = True\n",
        "    #parameter\n",
        "\n",
        "    #gamma 값이 1인 이유= 직선식에서 ax+b 부분의 a부분이기 때문 초기화 값 당연히 1\n",
        "    #beta 값이 1인이유 = 직선식에서 ax+b 부분의 b부분의 초기화 값이기 때문\n",
        "    #gamma beta는 scale shift를 위한것 다양한 값을 선택하기 위함\n",
        "    #gaussian 분포의 x,y축으로의 움직임\n",
        "\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "\n",
        "    #buffer\n",
        "    #위와비슷 초기 mean이 0으로 선택되면 고려x\n",
        "    #variance또한 마찬가지 1로 설정되면 고려x\n",
        "    self.runningmean = torch.zeros(dim)\n",
        "    self.runningvar= torch.ones(dim)\n",
        "\n",
        "  def __call__(self,x):\n",
        "\n",
        "    if self.training:\n",
        "      xmean = x.mean(0,keepdim=True)  #batchmean\n",
        "      xvar =x.var(0,keepdim=True,unbiased=True)   #batchvariance\n",
        "    else:\n",
        "      xmean = self.runningmean\n",
        "      xvar= self.runningvar\n",
        "\n",
        "    xhat=(x-xmean)/torch.sqrt(self.eps+xvar) #actiavationm,batchnorm.ipynb\n",
        "    self.out = self.gamma*xhat + self.beta\n",
        "\n",
        "    #buffer 업데이트  momentum comes into play & gradient descent 영향을 받지않음\n",
        "    #부드럽게 업데이트할 수 있으며, 훈련 초기에는 높은 학습률을 유지함\n",
        "    #훈련이 진행될수록 이동 평균이 더 중요해지게 됨\n",
        "\n",
        "    #전체 데이터셋 평균,분산만 사용하는게 아니라 학습하며 얻은정보도 활용\n",
        "    if self.training:\n",
        "      with torch.no_grad():\n",
        "        self.runningmean =  (1-self.momentum)*self.runningmean + self.momentum*xmean\n",
        "        self.runningvar = (1-self.momentum)*self.runningvar + self.momentum*xvar\n",
        "    return self.out\n",
        "\n",
        "  def parameter(self):\n",
        "    return [self.gamma,self.beta]\n",
        "\n",
        "#----------------------------------------------------------------------------------\n",
        "\n",
        "class tanh:\n",
        "  def __call__(self,x):\n",
        "    self.out = torch.tanh(x)\n",
        "    return self.out\n",
        "\n",
        "  def parameter(self):\n",
        "    return []\n",
        "#----------------------------------------------------------------\n",
        "\n",
        "class embedding():\n",
        "  def __init__(self,n_embd,embedding_dim):\n",
        "    self.weight = torch.randn((n_embd,embedding_dim))\n",
        "    #C matrix\n",
        "\n",
        "  def __call__(self,x):\n",
        "    self.out = self.weight[x]\n",
        "    # C[xb]\n",
        "    return self.out\n",
        "\n",
        "  def parameter(self):\n",
        "    return [self.weight] #C를 param으로 설정\n",
        "\n",
        "#---------------------------------------------------------------\n",
        "\n",
        "class flatten():\n",
        "  def __call__(self,x):\n",
        "    self.out = x.view(x.shape[0],-1)\n",
        "    return self.out\n",
        "\n",
        "  def parameter(self):\n",
        "    return []\n",
        "\n",
        "#-----------------------------------------------------------------\n",
        "\n",
        "class sequential:\n",
        "  def __init__(self,layers):\n",
        "    self.layers = layers\n",
        "\n",
        "  def __call__(self,x):\n",
        "    for layer in self.layers:\n",
        "      x= layer(x)\n",
        "    self.out = layer\n",
        "    return self.out\n",
        "\n",
        "  def parameter(self):\n",
        "    return [p for layer in self.layers for p in layer.parameter()]\n",
        "\n",
        "#--------------------------------------------------------------------\n",
        "class flattenconsecutive:\n",
        "    def __init__(self, n):\n",
        "        # n is the number of consecutive elements we want (2 in our example)\n",
        "        self.n = n\n",
        "\n",
        "    def __call__(self, x):\n",
        "        # in our example: B = 5, T = 8, C = 10\n",
        "        B, T, C = x.shape\n",
        "        # we want to convert X to (5, 4, 20)\n",
        "        x = x.view(B, T // self.n, C * self.n)\n",
        "\n",
        "        if x.shape[1] == 1:\n",
        "            x = x.squeeze(1)\n",
        "\n",
        "        self.out = x\n",
        "        return self.out\n",
        "\n",
        "    def parameter(self):\n",
        "        return []"
      ],
      "metadata": {
        "id": "O97aN7vQY-_-"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_embd = 24\n",
        "n_hidden = 128\n",
        "\n",
        "model = sequential([\n",
        "  embedding(vocab_size, n_embd),\n",
        "  flattenconsecutive(2), Linear(n_embd * 2, n_hidden, bias=False), batchnorm1d(n_hidden), tanh(),\n",
        "  flattenconsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), batchnorm1d(n_hidden), tanh(),\n",
        "  flattenconsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), batchnorm1d(n_hidden), tanh(),\n",
        "  Linear(n_hidden, vocab_size),\n",
        "])\n",
        "\n",
        "with torch.no_grad():\n",
        "  model.layers[-1].weight *= 0.1\n",
        "\n",
        "parameters = model.parameter()\n",
        "print(sum(p.nelement() for p in parameters))\n",
        "for p in parameters:\n",
        "  p.requires_grad = True\n",
        "\n",
        "max_steps = 200000\n",
        "batch_size = 32\n",
        "lossi = []\n",
        "\n",
        "for i in range(max_steps):\n",
        "\n",
        "  # minibatch construct\n",
        "  ix = torch.randint(0, xtr.shape[0], (batch_size,))\n",
        "  Xb, Yb = xtr[ix], ytr[ix] # batch X,Y\n",
        "\n",
        "  # forward pass\n",
        "  logits = model(Xb)\n",
        "  loss = F.cross_entropy(logits, Yb) # loss function\n",
        "\n",
        "  # backward pass\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "  loss.backward()\n",
        "\n",
        "  # update: simple SGD\n",
        "  lr = 0.1 if i < 150000 else 0.01 # step learning rate decay\n",
        "  for p in parameters:\n",
        "    p.data += -lr * p.grad\n",
        "\n",
        "  # track stats\n",
        "  if i % 10000 == 0: # print every once in a while\n",
        "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
        "  lossi.append(loss.log10().item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "1ID2bqCBPbHK",
        "outputId": "054b4d58-0d67-4caf-e7a1-6a8c0aef1df9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "76579\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "cross_entropy_loss(): argument 'input' (position 1) must be Tensor, not Linear",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-f87cf25bc9ad>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m   \u001b[0;31m# forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# loss function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[0;31m# backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3051\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3052\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3053\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3055\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: cross_entropy_loss(): argument 'input' (position 1) must be Tensor, not Linear"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sampling from the model\n",
        "g = torch.Generator().manual_seed(2147483647 + 10)\n",
        "\n",
        "for _ in range(20):\n",
        "    out = []\n",
        "    context = [0] * block_size\n",
        "    while True:\n",
        "        # Forward pass\n",
        "        logits = model(torch.tensor([context]).reshape(1, -1))\n",
        "        probs = F.softmax(logits, dim = 1)\n",
        "\n",
        "        ix = torch.multinomial(probs, num_samples = 1).item()\n",
        "\n",
        "        # Shift the Context Window\n",
        "        context = context[1:] + [ix]\n",
        "\n",
        "        if ix == 0:\n",
        "            break\n",
        "\n",
        "        out.append(ix)\n",
        "\n",
        "    print(\"\".join(itos[i] for i in out))"
      ],
      "metadata": {
        "id": "g1fZ4uChZW88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(C.shape)\n",
        "#print([p.shape for layer in layers for p in layer.parameter()])\n",
        "#numpy broadcasting 덧셈,곱셈"
      ],
      "metadata": {
        "id": "5juU8byIuzaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "max_step = 20000\n",
        "batchsize = 32\n",
        "\n",
        "lossi = []\n",
        "\n",
        "for i in range(max_step):\n",
        "\n",
        "# 미니배치 분리 randint 사용해서 random으로 쓸 xtr 가져옴\n",
        "  idx = torch.randint(0,xtr.shape[0],(batchsize,))\n",
        "  xb,yb = xtr[idx],ytr[idx]\n",
        "\n",
        "  #xtr -> xb\n",
        "  #shape = [전체 문자,block_size] -> 미니배치 -> [minibatch_size,block_size]\n",
        "\n",
        "  #class 로 embedding flatten정의했기때문에 layer안에 정의되있음\n",
        "  #emb = C[xb] # emb matrix C에 | [[1,14,6],...] 문자 xb 임베딩 과정\n",
        "  #x= emb.view(emb.shape[0],-1) #layers 첫 hidden layer차원과 맞추는 과정 = 임베딩\n",
        "  \n",
        "  #------------------------\n",
        "  #x= xb\n",
        "  #for layer in layers:\n",
        "  #  x=layer(x)\n",
        "  #-------------------------  \n",
        "  #         |\n",
        "  #         |\n",
        "  #         V\n",
        "  \n",
        "  logits = model(xb)\n",
        "  loss = F.cross_entropy(logits,yb)\n",
        "\n",
        "  #backward pass\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "  loss.backward()\n",
        "\n",
        "  lr = 0.1 if i < 150000 else 0.01\n",
        "  for p in parameters:\n",
        "    p.data += -lr * p.grad\n",
        "\n",
        "  if i % 10000 == 0:\n",
        "    print(f'{i:6d}/{max_step:6d}:{loss.item():.3f}')\n",
        "  lossi.append(loss.log10().item())\n"
      ],
      "metadata": {
        "id": "YYEUqw8ym4nM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#layers 를 더 훈련시키지 않는다 -> eval mode\n",
        "for layer in model.layers:\n",
        "  layer.training =False"
      ],
      "metadata": {
        "id": "UhZ3xymeG_MT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#,,,eval mode down below"
      ],
      "metadata": {
        "id": "U7hApizgHQId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logit.shape"
      ],
      "metadata": {
        "id": "lT9Neb9KH6On"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#만든 모델 실행\n",
        "for i in range(20):\n",
        "  out = []\n",
        "  context= [0]*block_size\n",
        "  while True:\n",
        "    logits = model(torch.tensor([context]))\n",
        "    probs = F.softmax(logits,dim=1)"
      ],
      "metadata": {
        "id": "w9WrcS1BHb-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(20):\n",
        "    out = []\n",
        "    context = [0] * block_size\n",
        "    while True:\n",
        "        # Forward pass\n",
        "        logits = model(torch.tensor([context]).reshape(1, -1))\n",
        "        probs = F.softmax(logits, dim = 1)\n",
        "\n",
        "        ix = torch.multinomial(probs, num_samples = 1).item()\n",
        "\n",
        "        # Shift the Context Window\n",
        "        context = context[1:] + [ix]\n",
        "\n",
        "        if ix == 0:\n",
        "            break\n",
        "\n",
        "        out.append(ix)\n",
        "\n",
        "    print(\"\".join(itos[i] for i in out))"
      ],
      "metadata": {
        "id": "odSnpCJQXZl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(torch.tensor(lossi).view(-1,1000))"
      ],
      "metadata": {
        "id": "HcLAwhulyF7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(torch.tensor(lossi).view(-1,1000).mean(1))"
      ],
      "metadata": {
        "id": "6cIOXAlenNX9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}